{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "552ec552",
      "metadata": {
        "id": "552ec552"
      },
      "source": [
        "# SetFit for Text Classification\n",
        "https://arxiv.org/pdf/2209.11055.pdf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3af7f258-5aaf-47c2-b81e-2f10fc349812",
      "metadata": {
        "id": "3af7f258-5aaf-47c2-b81e-2f10fc349812"
      },
      "source": [
        "In this notebook, we'll learn how to do:\n",
        "- few-shot text classification with SetFit\n",
        "- distillate a model\n",
        "- quantize a model\n",
        "- zero-shot text classification\n",
        "\n",
        "\n",
        "Please note that this notebook is GREATLY inspired from the notebooks available in the official [repository](https://github.com/huggingface/setfit/tree/main/notebooks).\n",
        "Here we combine some of them to have an overview of SetFit."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5604f73-f395-42cb-8082-9974a87ef9e9",
      "metadata": {
        "id": "c5604f73-f395-42cb-8082-9974a87ef9e9"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26f09e23-2e1f-41f6-bb40-a30d447a0541",
      "metadata": {
        "id": "26f09e23-2e1f-41f6-bb40-a30d447a0541"
      },
      "source": [
        "If you're running this Notebook on Colab or some other cloud platform, you will need to install a few libraries.\n",
        "Uncomment the following cell and run it to install the requirements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "712b96e8",
      "metadata": {
        "id": "712b96e8"
      },
      "outputs": [],
      "source": [
        "%pip install setfit\n",
        "%pip install datasets\n",
        "%pip install pandas\n",
        "%pip install scikit-learn\n",
        "%pip install onnx\n",
        "%pip install optimum\n",
        "%pip install onnxruntime\n",
        "%pip install neural_compressor\n",
        "%pip install evaluate\n",
        "%pip install huggingface_hub\n",
        "%pip install matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78255442",
      "metadata": {
        "id": "78255442"
      },
      "source": [
        "Please relaunch the kernel after installing the libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b2a8fcd-46fe-43e4-835e-57b84964358a",
      "metadata": {
        "id": "2b2a8fcd-46fe-43e4-835e-57b84964358a"
      },
      "source": [
        "This notebook is designed to work with any multiclass [text classification dataset](https://huggingface.co/models?pipeline_tag=text-classification&sort=downloads) and pretrained [Sentence Transformer](https://huggingface.co/models?library=sentence-transformers&sort=downloads) on the Hub. Change the values below to try a different dataset / model!\n",
        "\n",
        "Here we will use the all-mpnet-base-v2 that gives the best quality on the current sentence transformers open source datasets\n",
        "See https://www.sbert.net/docs/pretrained_models.html for more information"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e756be8-3b60-4c86-aa1b-7ef78289b8e2",
      "metadata": {
        "id": "3e756be8-3b60-4c86-aa1b-7ef78289b8e2"
      },
      "source": [
        "## Loading and sampling the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc78c620",
      "metadata": {
        "id": "cc78c620"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display, Markdown\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import datasets\n",
        "from datasets import Dataset, load_dataset, concatenate_datasets\n",
        "from setfit import SetFitModel\n",
        "#from setfit.data import concatenate_datasets\n",
        "\n",
        "\n",
        "def create_dataset(input_file):\n",
        "    \"\"\"Create dataset from local file\"\"\"\n",
        "    # Load raw dataset\n",
        "    df = pd.read_csv(input_file)\n",
        "    labels = sorted(list(df[\"label\"].unique()))\n",
        "    \n",
        "    # Prepare the dataset\n",
        "    dataset_dict = {\n",
        "        'idx': df.index,\n",
        "        'sentence': df[\"text\"],\n",
        "        'label': df[\"label\"]\n",
        "    }\n",
        "    features = datasets.Features({\n",
        "        'idx': datasets.Value('int32'),\n",
        "        'sentence': datasets.Value('string'),\n",
        "        'label': datasets.ClassLabel(num_classes=len(labels), names=labels),\n",
        "    })\n",
        "\n",
        "    # Create dataset in HF dataset format\n",
        "    dataset = datasets.Dataset.from_dict(dataset_dict, features, split=\"train\")\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def evaluate_more_metrics(trainer):\n",
        "    \"\"\"Create a classification report using a trainer object\"\"\"\n",
        "    trainer._validate_column_mapping(trainer.eval_dataset)\n",
        "    eval_dataset = trainer.eval_dataset\n",
        "\n",
        "    if trainer.column_mapping is not None:\n",
        "        eval_dataset = trainer._apply_column_mapping(trainer.eval_dataset, trainer.column_mapping)\n",
        "\n",
        "    x_test = eval_dataset[\"text\"]\n",
        "    y_test = eval_dataset[\"label\"]\n",
        "\n",
        "    y_pred = trainer.model.predict(x_test)\n",
        "    print(sklearn.metrics.classification_report(y_test, y_pred))\n",
        "    return y_test, y_pred\n",
        "\n",
        "\n",
        "def sample_dataset(dataset: Dataset, label_column: str = \"label\", num_samples: int = 8, seed: int = 42, class_weights = None) -> Dataset:\n",
        "    \"\"\"Samples a Dataset to create an equal number of samples per class (when possible).\"\"\"\n",
        "    \n",
        "    # Shuffle the dataset \n",
        "    shuffled_dataset = dataset.shuffle(seed=seed)\n",
        "    \n",
        "    samples = []\n",
        "    # Get unique labels\n",
        "    num_labels =  dataset.unique(label_column)\n",
        "    \n",
        "    # For each label\n",
        "    for label in num_labels:\n",
        "        data = shuffled_dataset.filter(lambda example: example[label_column] == label)\n",
        "        # Get the number of data to sample based on the class_weights\n",
        "        num_label_samples = min(len(data), num_samples * class_weights[label])\n",
        "        # Add sample\n",
        "        samples.append(data.select([i for i in range(num_label_samples)]))\n",
        "    \n",
        "    # Concatenate all the samples\n",
        "    all_samples = concatenate_datasets(samples)\n",
        "    # Shuffle the samples\n",
        "    return all_samples.shuffle(seed=seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35e359e2",
      "metadata": {
        "id": "35e359e2"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "from pathlib import Path\n",
        "from time import perf_counter\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "class PerformanceBenchmark:\n",
        "    def __init__(self, model, dataset, optim_type, metric_monitor):\n",
        "        \"\"\"Initiatilize the Performance Benchmark\"\"\"\n",
        "        self.model = model\n",
        "        self.dataset = dataset\n",
        "        self.optim_type = optim_type\n",
        "        self.metric = evaluate.load(metric_monitor)\n",
        "        self.metric_monitor = metric_monitor\n",
        "        \n",
        "\n",
        "    def compute_metric(self):\n",
        "        \"\"\"Compute the self.metric for the the self.dataset \"\"\"\n",
        "        preds = self.model.predict(self.dataset[\"sentence\"])\n",
        "        labels = self.dataset[\"label\"]\n",
        "        args = {\"predictions\": preds, \"references\": labels}\n",
        "        if self.metric_monitor == \"f1\":\n",
        "            args[\"average\"] = \"macro\"\n",
        "        #metric_compute = self.metric.compute(predictions=preds, references=labels)\n",
        "        metric_compute = self.metric.compute(**args)\n",
        "        print(f\"{self.metric_monitor} on test set - {metric_compute[self.metric_monitor]:.3f}\")\n",
        "        return metric_compute\n",
        "\n",
        "    def compute_size(self):\n",
        "        \"\"\"Compute the size of the self.model\"\"\"\n",
        "        state_dict = self.model.model_body.state_dict()\n",
        "        tmp_path = Path(\"model.pt\")\n",
        "        torch.save(state_dict, tmp_path)\n",
        "        # Calculate size in megabytes\n",
        "        size_mb = Path(tmp_path).stat().st_size / (1024 * 1024)\n",
        "        # Delete temporary file\n",
        "        tmp_path.unlink()\n",
        "        print(f\"Model size (MB) - {size_mb:.2f}\")\n",
        "        return {\"size_mb\": size_mb}\n",
        "\n",
        "    def time_model(self, query=\"What is the pin number for my account?\", nb_warmup=10, nb_inference= 100):\n",
        "        \"\"\"Compute the average inference time of the self.model\"\"\"\n",
        "        latencies = []\n",
        "        # Warmup using nb_warmup requests\n",
        "        # The first inference usually takes more time because of the loading\n",
        "        # of the model in the memory\n",
        "        for _ in range(nb_warmup):\n",
        "            _ = self.model([query])\n",
        "        # Timed run using nb_inference requests\n",
        "        # To have significant results we launch the same request nb_inference tims\n",
        "        for _ in range(nb_inference):\n",
        "            start_time = perf_counter()\n",
        "            _ = self.model([query])\n",
        "            latencies.append(perf_counter() - start_time)\n",
        "        # Compute run statistics in ms\n",
        "        time_avg_ms = 1000 * np.mean(latencies)\n",
        "        time_std_ms = 1000 * np.std(latencies)\n",
        "        print(f\"Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f}\")\n",
        "        return {\"time_avg_ms\": time_avg_ms, \"time_std_ms\": time_std_ms}\n",
        "\n",
        "    def run_benchmark(self):\n",
        "        \"\"\" Launch the benchmark (compute size, metric and inference time)\"\"\"\n",
        "        metrics = {}\n",
        "        metrics[self.optim_type] = self.compute_size()\n",
        "        metrics[self.optim_type].update(self.compute_metric())\n",
        "        metrics[self.optim_type].update(self.time_model())\n",
        "        return metrics\n",
        "\n",
        "\n",
        "def plot_metrics(perf_metrics, current_optim_type, metric_monitor):\n",
        "    \"\"\" Plot the metrics (time x metric)\"\"\"\n",
        "    \n",
        "    # Create perf_metrics dataset\n",
        "    df = pd.DataFrame.from_dict(perf_metrics, orient=\"index\")\n",
        "    \n",
        "    metric_range_values = []\n",
        "    # Loop over the dataframe\n",
        "    for idx in df.index:\n",
        "        df_opt = df.loc[idx]\n",
        "        metric_range_values.append(df_opt[metric_monitor] * 100)\n",
        "        # Print a point corresponding to (time, metric)\n",
        "        plt.scatter(\n",
        "                df_opt[\"time_avg_ms\"],\n",
        "                df_opt[metric_monitor] * 100,\n",
        "                alpha=0.5,\n",
        "                s=df_opt[\"size_mb\"],\n",
        "                label=idx,\n",
        "                # Add a dashed circle around the current optimization type\n",
        "                marker=\"$\\u25CC$\" if idx == current_optim_type else None,\n",
        "            )\n",
        "\n",
        "    # Add legend\n",
        "    legend = plt.legend(bbox_to_anchor=(1, 1))\n",
        "    for handle in legend.legendHandles:\n",
        "        handle.set_sizes([20])\n",
        "\n",
        "    # Adapt y axis with the range values\n",
        "    plt.ylim(max(-1, min(metric_range_values) - 10), 95)\n",
        "    # Use the slowest model to define the x-axis range\n",
        "    xlim = int(max(df[\"time_avg_ms\"]) + 5)\n",
        "    \n",
        "    plt.xlim(0, xlim)\n",
        "    plt.ylabel(f\"{metric_monitor} (%)\")\n",
        "    plt.xlabel(\"Average latency (ms)\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f653ac4e",
      "metadata": {
        "id": "f653ac4e"
      },
      "source": [
        "We will use the 🤗 Datasets library to download the data, however note that you can use any other dataset in a csv format with two columns ('label' and 'text')\n",
        "\n",
        "Most datasets on the Hub have many more labeled examples than those one encounters in few-shot settings. To simulate the effect of training on a limited number of examples, let's subsample the training set to have 8 labeled examples per class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba671e5e-58f7-4d9e-aa82-8c0413b4a8df",
      "metadata": {
        "id": "ba671e5e-58f7-4d9e-aa82-8c0413b4a8df"
      },
      "outputs": [],
      "source": [
        "## Dataset choice\n",
        "# Choose any of the available text classification dataset below.\n",
        "# Note that you can make it work with any text classification dataset available in https://huggingface.co/datasets?pipeline_tag=text-classification\n",
        "datasets_available = {\n",
        "    # sentiment analysis pos + neg\n",
        "    \"sst2\": {\"datasets\": {\"train\":\"train\", \"validation\": \"validation\"}, \"mapping\":{\"sentence\": \"text\", \"label\": \"label\"}},\n",
        "    # sentiment analysis movie revies pos + neg    \n",
        "    \"imdb\": {\"datasets\":{\"train\":\"train\", \"validation\": \"test\"}, \"mapping\":{\"sentence\": \"text\", \"label\": \"label\"}},\n",
        "    # entailment (0), neutral (1), contradiction (2)\n",
        "    \"anli\": {\"datasets\": {\"train\":\"train_r1\", \"validation\": \"dev_r1\"}, \"mapping\":{\"sentence\": \"premise\", \"label\": \"label\"}},\n",
        "    # sentiment analysis, movies reviews pos + neg\n",
        "    \"rotten_tomatoes\": {\"datasets\": {\"train\":\"train\", \"validation\": \"validation\"}, \"mapping\":{\"sentence\": \"text\", \"label\": \"label\"}},\n",
        "    # sentiment analysis, non toxic + toxic\n",
        "    \"mteb/toxic_conversations_50k\": {\"datasets\": {\"train\":\"train\", \"validation\": \"test\"}, \"mapping\":{\"sentence\": \"text\", \"label\": \"label\"}},\n",
        "    # language identification (20 languages)\n",
        "    # arabic (ar), bulgarian (bg), german (de), modern greek (el), english (en), spanish (es), french (fr), hindi (hi), italian (it), japanese (ja), dutch (nl), polish (pl), portuguese (pt), russian (ru), swahili (sw), thai (th), turkish (tr), urdu (ur), vietnamese (vi), and chinese (zh)\n",
        "    \"papluca/language-identification\": {\"datasets\": {\"train\":\"train\", \"validation\": \"test\"}, \"mapping\":{\"sentence\": \"text\", \"label\": \"label\"}},\n",
        "    # topic 14 classes\n",
        "    # too long\n",
        "    \"dbpedia_14\": {\"datasets\": {\"train\":\"train\", \"validation\": \"test\"}, \"mapping\":{\"sentence\": \"content\", \"label\": \"label\"}}    \n",
        "}\n",
        "\n",
        "DATASET_ID = \"sst2\"\n",
        "\n",
        "\n",
        "## Dataset parameters\n",
        "USE_LOCAL_DATASET = False\n",
        "TRAIN = \"TODO\"\n",
        "VAL = \"TODO\"\n",
        "USE_CLASS_WEIGHTS = False # Put more examples in the subset for training\n",
        "NUM_SAMPLES = 8 # default to 8, but try to make this vary to observe the results\n",
        "\n",
        "## Model parameters\n",
        "# Here are the models that will be used, for the purpose of this workshop\n",
        "# we should not modify those however you can try by using models from the hub\n",
        "# to compare.\n",
        "MODEL_ID = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "\n",
        "# Monitor parameters\n",
        "# This is the metric to monitor, here we will use the f1 score\n",
        "# more suited for unbalanced datasets\n",
        "METRIC_MONITOR = \"accuracy\"\n",
        "\n",
        "\n",
        "# Load datasets\n",
        "if USE_LOCAL_DATASET:\n",
        "    to_disp = f'Here we use a local dataset from the files {TRAIN} and {VAL}'\n",
        "    # If you use a local dataset make sure that you have two columns: 'text' and 'label'\n",
        "    train_dataset = create_dataset(TRAIN)\n",
        "    eval_dataset = create_dataset(VAL)\n",
        "else:\n",
        "    to_disp = f'Here we use the dataset {DATASET_ID}'\n",
        "    dataset = load_dataset(DATASET_ID)\n",
        "    train_dataset = dataset[datasets_available[DATASET_ID][\"datasets\"][\"train\"]]\n",
        "    eval_dataset = dataset[datasets_available[DATASET_ID][\"datasets\"][\"validation\"]]\n",
        "    if \"label\" not in train_dataset.features:\n",
        "        train_dataset = train_dataset.rename_column(datasets_available[DATASET_ID][\"mapping\"][\"sentence\"], \"sentence\")\n",
        "        eval_dataset = eval_dataset.rename_column(datasets_available[DATASET_ID][\"mapping\"][\"label\"], \"label\")\n",
        "    if \"sentence\" not in train_dataset.features:\n",
        "        train_dataset = train_dataset.rename_column(datasets_available[DATASET_ID][\"mapping\"][\"sentence\"], \"sentence\")\n",
        "        eval_dataset = eval_dataset.rename_column(datasets_available[DATASET_ID][\"mapping\"][\"sentence\"], \"sentence\")\n",
        "\n",
        "# Load the metric to monitor\n",
        "metric_monitor = evaluate.load(METRIC_MONITOR)\n",
        "\n",
        "# Get class weights\n",
        "vc = pd.Series(train_dataset[\"label\"]).value_counts()\n",
        "class_weights = {x: 1 for x in vc.index}\n",
        "if USE_CLASS_WEIGHTS:\n",
        "    class_weights = {x: int(y) for x,y in (vc / min(vc)).items()}\n",
        "\n",
        "# New function to sample dataset\n",
        "train_dataset_sampled = sample_dataset(train_dataset, num_samples=NUM_SAMPLES, class_weights=class_weights)\n",
        "\n",
        "to_disp += f' we have {len(train_dataset_sampled)} total examples to train with since dataset has {len(set(train_dataset[\"label\"]))} classes'\n",
        "if USE_CLASS_WEIGHTS:\n",
        "    to_disp += f' and we sample the following for each class: {class_weights}'\n",
        "else:\n",
        "    to_disp += f' and we sample {NUM_SAMPLES} per class'\n",
        "display(Markdown(to_disp))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "298d31c4",
      "metadata": {
        "id": "298d31c4"
      },
      "source": [
        "We can do a small data analysis to observe our data:\n",
        "- Display some raw examples\n",
        "- Display the number of examples by label\n",
        "You can do that on the `train_data` and `train_data_sampled` to see if the ratios are similar or if it respects the class_weights indicated.\n",
        "Have a look at the `train_data_sampled` and check if there seems to be enough diversity in the input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f1661ed",
      "metadata": {
        "id": "7f1661ed"
      },
      "outputs": [],
      "source": [
        "def data_analysis(dataset, name=\"\", nb_examples_per_class=2):\n",
        "    \"\"\" Create the data analysis \"\"\"\n",
        "    display(f'Data analysis of {name}')\n",
        "    du = pd.DataFrame()\n",
        "    du[\"label\"] = dataset[\"label\"]\n",
        "    du[\"sentence\"] = dataset[\"sentence\"]\n",
        "    # Display the label distriubtion\n",
        "    display(du[\"label\"].value_counts() / len(du[\"label\"]))\n",
        "\n",
        "    # Display nb_examples_per _class\n",
        "    indices = []\n",
        "    for l in du[\"label\"].unique():\n",
        "        for idx in du[du[\"label\"]==l].sample(nb_examples_per_class).index:\n",
        "            indices.append(idx)\n",
        "    display(du[du.index.isin(indices)])\n",
        "\n",
        "    # Return the dataframe\n",
        "    return du\n",
        "\n",
        "# Display nb_examples per class\n",
        "nb_examples_per_class = 4\n",
        "# TODO: Launch data analysis for train_dataset\n",
        "_ = data_analysis(train_dataset, name=\"train_dataset\", nb_examples_per_class=nb_examples_per_class)\n",
        "#  TODO: Launch data analysis for train_dataset_sampled\n",
        "du  = data_analysis(train_dataset_sampled, name=\"train_dataset_sampled\", nb_examples_per_class=nb_examples_per_class)\n",
        "\n",
        "\n",
        "# Show the train_dataset_sampled\n",
        "display(\"Display the whole train_dataset_sampled\")\n",
        "display(du)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37e7c839-1f06-4d35-aa34-6e13659db814",
      "metadata": {
        "id": "37e7c839-1f06-4d35-aa34-6e13659db814"
      },
      "source": [
        "Okay, now that we have the dataset, let's load and train a model!\n",
        "## Fine-tuning the model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78e8c41a",
      "metadata": {
        "id": "78e8c41a"
      },
      "source": [
        "To train a SetFit model, the first thing to do is download a pretrained checkpoint from the Hub. We can do so by using the `from_pretrained()` method associated with the `SetFitModel` class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33661c9d-46d3-42eb-9b15-8a2bc49d7f6c",
      "metadata": {
        "id": "33661c9d-46d3-42eb-9b15-8a2bc49d7f6c"
      },
      "outputs": [],
      "source": [
        "# TODO: create a model using SetFitModel.from_pretrained using MODEL_ID\n",
        "model = SetFitModel.from_pretrained(MODEL_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84e7521e-95ca-431a-8d7f-2f18e1de16ce",
      "metadata": {
        "id": "84e7521e-95ca-431a-8d7f-2f18e1de16ce"
      },
      "source": [
        "Here, we've downloaded a pretrained Sentence Transformer from the Hub and added a logistic classification head to the create the SetFit model. As indicated in the message, we need to train this model on some labeled examples. We can do so by using the `SetFitTrainer` class as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e44b7069-27b0-49ea-bc27-c44f94a98e2f",
      "metadata": {
        "id": "e44b7069-27b0-49ea-bc27-c44f94a98e2f"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers.losses import CosineSimilarityLoss\n",
        "from setfit import SetFitTrainer\n",
        "\n",
        "# TODO create the  SetFitTrainer using the\n",
        "# model, the train_dataset, eval_dataset, the CosineSimilarityLoss,\n",
        "# We set the num_iterations to 20.\n",
        "# You can play with the num_epochs to observe the results, the default one is 1\n",
        "# The column_mapping is set to {\"sentence\": \"text\", \"label\": \"label\"}\n",
        "trainer = SetFitTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset_sampled,\n",
        "    eval_dataset=eval_dataset,\n",
        "    loss_class=CosineSimilarityLoss,\n",
        "    num_iterations=20,\n",
        "    num_epochs=1,\n",
        "    column_mapping={\"sentence\": \"text\", \"label\": \"label\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbd3e642",
      "metadata": {
        "id": "cbd3e642"
      },
      "source": [
        "The main arguments to notice in the trainer is the following:\n",
        "\n",
        "* `loss_class`: The loss function to use for contrastive learning with the Sentence Transformer body\n",
        "* `num_iterations`: The number of text pairs to generate for contrastive learning\n",
        "* `column_mapping`: The `SetFitTrainer` expects the inputs to be found in a `text` and `label` column. This mapping automatically formats the training and evaluation datasets for us."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6e3c5ae-c287-4936-b1ac-5eca10c7f39c",
      "metadata": {
        "id": "b6e3c5ae-c287-4936-b1ac-5eca10c7f39c"
      },
      "source": [
        "Now that we've created a trainer, we can train it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15a2e4f0",
      "metadata": {
        "id": "15a2e4f0"
      },
      "outputs": [],
      "source": [
        "# TODO: Launch the trainiing\n",
        "# The trainer has a train method\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7aea571b",
      "metadata": {
        "id": "7aea571b"
      },
      "source": [
        "The next step is to compute the model's performance using the `evaluate_more_metrics()` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d79e13b-37b1-4448-a7be-2bcc243b859d",
      "metadata": {
        "id": "3d79e13b-37b1-4448-a7be-2bcc243b859d"
      },
      "outputs": [],
      "source": [
        "# TODO: Evaluate the metrics on the trainer\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71e38b9e",
      "metadata": {
        "id": "71e38b9e"
      },
      "source": [
        "The final step is to compute to run a benchmark for our model to get the avg inference time and plot it with its metric to monitor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "262d3295",
      "metadata": {
        "id": "262d3295"
      },
      "outputs": [],
      "source": [
        "# TODO: Create a Performance Benchmark using\n",
        "# the model trained (trainer.model), the dataset that will be the eval_dataset\n",
        "# optim_type ise the name that we want to display (here MNPNET) and the metric_monitor that is METRIC_MONITOR\n",
        "# Create benchmark\n",
        "optim_type = ...\n",
        "pb = PerformanceBenchmark(\n",
        "    model=..., dataset=..., optim_type=..., metric_monitor=...\n",
        ")\n",
        "# TODO then we can run the benchmark using the function run_benchmark of the pb object\n",
        "perf_metrics = ...\n",
        "\n",
        "# TODO plot the metrics using the plot_metrics  that takes the perf_metrics as input, the optim_type and then METRIC_MONITOR\n",
        "# Plot metrics\n",
        "plot_metrics(..., ..., ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e799f994",
      "metadata": {
        "id": "e799f994"
      },
      "source": [
        "You can try it the setfit finetuned model on some specific examples.\n",
        "The following examples are used to classify the sentimeent analysis of the."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8687ffc",
      "metadata": {
        "id": "d8687ffc"
      },
      "outputs": [],
      "source": [
        "examples = [\n",
        "    \"i loved the spiderman movie!\",\n",
        "    \"pineapple on pizza is the worst 🤮\",\n",
        "    \"I care for you very much\",\n",
        "    \"I hate this\",\n",
        "    \"Fuck you\",\n",
        "    \"Asshole\",\n",
        "    \"You little piece of shit\",\n",
        "    \"I am very happy\"\n",
        "]\n",
        "# Launch the predictions\n",
        "preds = trainer.model(examples)\n",
        "\n",
        "# Display the results\n",
        "df = pd.DataFrame()\n",
        "df[\"example\"] = examples\n",
        "df[\"prediction\"] = preds\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03771d31",
      "metadata": {
        "id": "03771d31"
      },
      "source": [
        "# Distillation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a387c783",
      "metadata": {
        "id": "a387c783"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers.losses import CosineSimilarityLoss\n",
        "from setfit import SetFitModel, SetFitTrainer, DistillationSetFitTrainer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae978bbb",
      "metadata": {
        "id": "ae978bbb"
      },
      "source": [
        "Very often we have constraints in terms of resources:\n",
        "- inference time, the less the better for real time usage\n",
        "- memory usage, the less the better for reducing the cost of usage of our model\n",
        "\n",
        "One way of reducing both of those metrics is to do something called distillation.\n",
        "\n",
        "Model distillation, also known as knowledge distillation, is a technique used in machine learning to compress the knowledge of a large, complex model (called the teacher model) into a smaller, simpler model (called the student model). The purpose of this process is to create a more efficient model that retains much of the performance of the original, larger model while reducing computational resources, memory requirements, and inference time.\n",
        "\n",
        "For more information on distillation please check the following links:\n",
        "- https://medium.com/nlplanet/a-model-distillation-survey-7f0e1b56b3cf\n",
        "- https://neptune.ai/blog/knowledge-distillation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8075a88",
      "metadata": {
        "id": "d8075a88"
      },
      "source": [
        "In this workshop:\n",
        "- we use Relation-based knowledge distillation, we learn the embedding space of our smaller model using the embedding trained of our teacher and at the same time the logits of the teacher\n",
        "- for our teacher, the same architecture as before MPNet\n",
        "- for our student, a model called MiniLM-L3, a way smaller model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7810ab83",
      "metadata": {
        "id": "7810ab83"
      },
      "outputs": [],
      "source": [
        "# Model of the student\n",
        "# TODO: Here we use a \"paraphrase-MiniLM-L3-v2\"  as MODEL_STUDENT_ID\n",
        "MODEL_STUDENT_ID = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d417ff49",
      "metadata": {
        "id": "d417ff49"
      },
      "source": [
        "We first train two baselines:\n",
        "- a model based on MPNet\n",
        "- a model based one MiniLM-L3 without distillation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "146ca8b6",
      "metadata": {
        "id": "146ca8b6"
      },
      "outputs": [],
      "source": [
        "column_mapping = {\"sentence\": \"text\", \"label\": \"label\"}\n",
        "\n",
        "# TODO Get the model from the MODEL_STUDENT_ID using SetFitModel.from_pretrained\n",
        "minilm = ...\n",
        "\n",
        "# TODO: Prepare the dataset for the teacher  using the train_dataset, the NUM_SAMPLES and the class_weights defined before\n",
        "train_dataset_teacher = sample_dataset(..., num_samples=..., class_weights=...)\n",
        "\n",
        "# TODO: Create smaller model and train it the same way using\n",
        "# the minilm on the train_dataset_teacher, evaluating on the eval_dataset,\n",
        "# using the CosineSimilarityLoss and the column_mapping defined before\n",
        "minilm_trainer = SetFitTrainer(\n",
        "    model=...,\n",
        "    train_dataset=...,\n",
        "    eval_dataset=...,\n",
        "    loss_class=...\n",
        "    column_mapping=...\n",
        ")\n",
        "\n",
        "# TODO Train the minilm model using the train() method of minilm_trainer\n",
        "...\n",
        "\n",
        "\n",
        "####### TEACHER MODEL ####### \n",
        "# TODO Get the model from the MODEL_ID using SetFitModel.from_pretrained\n",
        "teacher_model = ...\n",
        "\n",
        "# TODO: Create smaller model and train it the same way using\n",
        "# the teacher_model ont the train_dataset_teacher, evaluating on the eval_dataset,\n",
        "# using the CosineSimilarityLoss and the column_mapping defined before\n",
        "teacher_trainer = SetFitTrainer(\n",
        "    model=...,\n",
        "    train_dataset=...,\n",
        "    eval_dataset=...,\n",
        "    loss_class=...,\n",
        "    column_mapping=...\n",
        ")\n",
        "# TODO Train the teacher model using the train() method of teacher_trainer\n",
        "...\n",
        "\n",
        "####### TEACHER MODEL #######"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b87dacb0",
      "metadata": {
        "id": "b87dacb0"
      },
      "source": [
        "After the training is done we can as before:\n",
        "- Evaluate both of our models\n",
        "- Launch a benchmark on both models\n",
        "- Plots the metrics obtained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b0f5bf7",
      "metadata": {
        "id": "7b0f5bf7"
      },
      "outputs": [],
      "source": [
        "# TODO Evalute the metrics using evaluate_more_metrics on the two models trained teacher_trainer and minilm_trainer\n",
        "y_true, y_pred = ...\n",
        "y_true, y_pred = ...\n",
        "\n",
        "# TODO Launch benchmark for both of our trained models teacher_trainer.model and minilm_trainer.model\n",
        "perf_metrics = {}\n",
        "for model_name, model in {\n",
        "    \"MPNet\": ...,\n",
        "    \"MiniLM-L3\": ...,\n",
        "    }.items():\n",
        "    display(model_name)\n",
        "    pb = PerformanceBenchmark(\n",
        "        model=model, dataset=eval_dataset, optim_type=model_name, metric_monitor=METRIC_MONITOR\n",
        "    )\n",
        "    res_metrics = pb.run_benchmark()\n",
        "    perf_metrics.update(res_metrics)\n",
        "\n",
        "# Plot metrics, we highlight \"MiniLM-L3\"\n",
        "plot_metrics(perf_metrics, \"MiniLM-L3\", metric_monitor=METRIC_MONITOR)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23eefeda",
      "metadata": {
        "id": "23eefeda"
      },
      "source": [
        "Next, we can distillate our teacher model into a model based on the smaller\n",
        "architecture (here MiniLM-L3)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d3795d1",
      "metadata": {
        "id": "2d3795d1"
      },
      "outputs": [],
      "source": [
        "# Number of data points to select from the train_dataset to train the student on\n",
        "DISTILLATION_TRAIN_SIZE = 500\n",
        "train_dataset_student = train_dataset.shuffle(seed=0).select(range(DISTILLATION_TRAIN_SIZE))\n",
        "\n",
        "####### STUDENT MODEL #######\n",
        "# TODO Load small student model using the MODEL_STUDENT_ID using SetFitModel.from_pretrained\n",
        "student_model = ...\n",
        "\n",
        "\n",
        "\n",
        "# Create trainer for knowledge distillation\n",
        "# TODO: Create the student trainer using\n",
        "# the teacher_model, the train_dataset_student for the train_dataset,\n",
        "# the student_model, evaluating on the eval_dataset,\n",
        "# using the CosineSimilarityLoss and the column_mapping defined before\n",
        "# You can play a bit with the different values\n",
        "student_trainer = DistillationSetFitTrainer(\n",
        "    teacher_model=...,\n",
        "    train_dataset=...,\n",
        "    student_model=...,\n",
        "    eval_dataset=eval_dataset,\n",
        "    loss_class=...,\n",
        "    column_mapping=...,\n",
        "    metric=METRIC_MONITOR,\n",
        "    batch_size=16,\n",
        "    num_iterations=20,\n",
        "    num_epochs=1,\n",
        ")\n",
        "# Train student with knowledge distillation\n",
        "\n",
        "# TODO: Launch the student training using the train() method of student_trained\n",
        "...\n",
        "####### STUDENT MODEL #######"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdc0d09d",
      "metadata": {
        "id": "cdc0d09d"
      },
      "source": [
        "The final step consists in:\n",
        "- Evaluating our student model\n",
        "- Benchmarking our student model\n",
        "- Adding its results to the perf_metrics\n",
        "- Plotting all the perf_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cb955da",
      "metadata": {
        "id": "0cb955da"
      },
      "outputs": [],
      "source": [
        "# Display evaluation for the distilled model\n",
        "# TODO Evalute the metrics on the student_trainer model using evaluate_more_metrics applied\n",
        "y_true, y_pred = evaluate_more_metrics(student_trainer)\n",
        "\n",
        "# TODO Launch benchmark for this distilled model student_trainer.model on the eval_dataset, with the optim_type and the METRIC_MONITOR for metric_monitor\n",
        "optim_type = \"MiniLM-L3 (distilled)\"\n",
        "pb = PerformanceBenchmark(\n",
        "    model=..., dataset=..., optim_type=..., metric_monitor=...\n",
        ")\n",
        "\n",
        "# TODO run the benchmark using run_benchmark of the pb object\n",
        "perf_metrics.update(...)\n",
        "\n",
        "# Plot metrics\n",
        "plot_metrics(perf_metrics, optim_type, metric_monitor=METRIC_MONITOR)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63d2b808",
      "metadata": {
        "id": "63d2b808"
      },
      "source": [
        "# Quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10bdc79f",
      "metadata": {
        "id": "10bdc79f"
      },
      "outputs": [],
      "source": [
        "from setfit.exporters.onnx import export_onnx"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcae3016",
      "metadata": {
        "id": "fcae3016"
      },
      "source": [
        "Quantization in machine learning is a technique used to reduce the memory and computational requirements of a model by approximating the continuous values of its parameters (e.g., weights and biases) with a smaller set of discrete values. This process can lead to more efficient models that consume less power and have lower latency, making them suitable for deployment on resource-constrained devices\n",
        "\n",
        "There are several types of quantization:\n",
        "- aware training, quantization done during the training\n",
        "- post training,\n",
        "    - dynamic, activations range are computed on the fly at runtime\n",
        "    - static, activations range are at quantization time (will be only used at runtime)\n",
        "dynamic is easy to implement, static is usually faster but with a drop in accuracy. We usually test with dynamic and if the results are okay we stick with it\n",
        "\n",
        "\n",
        "For more information on distillation please check the following links:\n",
        "- https://huggingface.co/docs/optimum/concept_guides/quantization\n",
        "- https://deci.ai/quantization-and-quantization-aware-training\n",
        "- https://github.com/intel/neural-compressor/blob/master/docs/source/quantization.md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c330ac4a",
      "metadata": {
        "id": "c330ac4a"
      },
      "outputs": [],
      "source": [
        "# TODO: Put a name to your model_student_quantized that will be pushed to huggingface hub\n",
        "MODEL_STUDENT_QUANTIZED = \"USERNAME/PROJET_NAME\"\n",
        "\n",
        "# Name of the quant conf file for quantization that will be dumped \n",
        "QUANT_CONF = \"MiniLM_L3_distilled_onnx_dynamic.yaml\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bce3162",
      "metadata": {
        "id": "4bce3162"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import functools\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from transformers import AutoTokenizer\n",
        "from setfit.exporters.utils import mean_pooling\n",
        "import onnxruntime\n",
        "from optimum.onnxruntime import ORTModelForFeatureExtraction\n",
        "#from optimum.pipelines import ORTModelForFeatureExtraction\n",
        "from neural_compressor.experimental import Quantization, common\n",
        "import shutil\n",
        "\n",
        "class OnnxSetFitModel:\n",
        "    def __init__(self, ort_model, tokenizer, model_head):\n",
        "        self.ort_model = ort_model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model_head = model_head\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        encoded_inputs = self.tokenizer(\n",
        "            inputs, padding=True, truncation=True, return_tensors=\"pt\"\n",
        "        )\n",
        "        outputs = self.ort_model(**encoded_inputs)\n",
        "        embeddings = mean_pooling(\n",
        "            outputs[\"last_hidden_state\"], encoded_inputs[\"attention_mask\"]\n",
        "        )\n",
        "        return self.model_head.predict(embeddings)\n",
        "\n",
        "    def __call__(self, inputs):\n",
        "        return self.predict(inputs)\n",
        "\n",
        "    \n",
        "class OnnxPerformanceBenchmark(PerformanceBenchmark):\n",
        "    def __init__(self, *args, model_path, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.model_path = model_path\n",
        "\n",
        "    def compute_size(self):\n",
        "        size_mb = Path(self.model_path).stat().st_size / (1024 * 1024)\n",
        "        print(f\"Model size (MB) - {size_mb:.2f}\")\n",
        "        return {\"size_mb\": size_mb}\n",
        "\n",
        "    def compute_metric(self):\n",
        "        preds = []\n",
        "        chunk_size = 100\n",
        "        for i in tqdm(range(0, len(self.dataset[\"text\"]), chunk_size)):\n",
        "            preds.extend(self.model.predict(self.dataset[\"text\"][i : i + chunk_size]))\n",
        "        labels = self.dataset[\"label\"]\n",
        "        args = {\"predictions\": preds, \"references\": labels}\n",
        "        if METRIC_MONITOR == \"f1\":\n",
        "            args[\"average\"] = \"macro\"\n",
        "        mm = metric_monitor.compute(**args)\n",
        "        print(f\"{METRIC_MONITOR} on test set - {mm[METRIC_MONITOR]:.3f}\")\n",
        "        return mm\n",
        "\n",
        "\n",
        "def eval_func(model, model_head=student_model.model_head):\n",
        "    ort_model = ORTModelForFeatureExtraction.from_pretrained(onnx_path)\n",
        "    ort_model.model = onnxruntime.InferenceSession(model.SerializeToString(), None)\n",
        "    onnx_setfit_model = OnnxSetFitModel(ort_model, tokenizer, model_head)\n",
        "    preds = []\n",
        "    chunk_size = 100\n",
        "    for i in tqdm(range(0, len(test_dataset[\"text\"]), chunk_size)):\n",
        "        preds.extend(\n",
        "            onnx_setfit_model.predict(test_dataset[\"text\"][i : i + chunk_size])\n",
        "        )\n",
        "    labels = test_dataset[\"label\"]\n",
        "    mm = metric_monitor.compute(predictions=preds, references=labels)\n",
        "    print(mm[METRIC_MONITOR])\n",
        "    return mm[METRIC_MONITOR]\n",
        "\n",
        "\n",
        "def build_dynamic_quant_yaml():\n",
        "    # 1% relative accuracy loss is set as the accuracy target for auto-tuning.\n",
        "    yaml = \"\"\"\n",
        "        model:\n",
        "          name: bert\n",
        "          framework: onnxrt_integerops\n",
        "\n",
        "        device: cpu\n",
        "\n",
        "        quantization:\n",
        "          approach: post_training_dynamic_quant\n",
        "\n",
        "        tuning:\n",
        "          accuracy_criterion:\n",
        "            relative: 0.01\n",
        "          exit_policy:\n",
        "            timeout: 0\n",
        "          random_seed: 9527\n",
        "    \"\"\"\n",
        "    # Here we quantize the model so that we have at most 0.01 difference in accuracy\n",
        "    with open(QUANT_CONF, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(yaml)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "251da5c3",
      "metadata": {
        "id": "251da5c3"
      },
      "source": [
        "After the results obtained on distillations we can analyze what the quantization can add in this whole pipeline. Basically the goal is to reduce the size of the model (on the same architecture) and raise the inference time.\n",
        "\n",
        "First we need to dump our model in onnx format. It is an open source format and the conversion is necessary to quantize using onnxruntime:\n",
        "- Get the model and tokenizer\n",
        "- Dump the model in onnx format in './onnx/model.onnx'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd3f02ee",
      "metadata": {
        "id": "cd3f02ee"
      },
      "outputs": [],
      "source": [
        "# TODO connect to your huggingface-cli in order to be able to upload a model\n",
        "# Get the token from https://huggingface.co/settings/tokens (write token) and replace TODO by its value\n",
        "!huggingface-cli login --token TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3eb13267",
      "metadata": {
        "id": "3eb13267"
      },
      "outputs": [],
      "source": [
        "# TODO Push to the hub your model using the push_to_hub function of the student_trainer with MODEL_STUDENT_QUANTIZED\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96f328ea",
      "metadata": {
        "id": "96f328ea"
      },
      "outputs": [],
      "source": [
        "# Path where to dump the model\n",
        "if os.path.exists(\"./onnx\"):\n",
        "    shutil.rmtree('./onnx')\n",
        "onnx_path = Path(\"onnx\")\n",
        "\n",
        "# Get the feature extractor part\n",
        "# TODO Load the model that you uploaded using MODEL_STUDENT_QUANTIZED\n",
        "ort_model = ORTModelForFeatureExtraction.from_pretrained(\n",
        "    ..., from_transformers=True\n",
        ")\n",
        "\n",
        "# TODO Get the tokenizer from the model in the hub MODEL_STUDENT_QUANTIZED\n",
        "tokenizer = AutoTokenizer.from_pretrained(...)\n",
        "\n",
        "# TODO: Save the model and the tokenizer to onnx_path using save_pretrained methods\n",
        "ort_model.save_pretrained(...)\n",
        "tokenizer.save_pretrained(...)\n",
        "model = ort_model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81d91a68",
      "metadata": {
        "id": "81d91a68"
      },
      "source": [
        "Then we launch the quantization of our model:\n",
        "- Load our model using the OnnxSetFitModel class\n",
        "- Prepare the quantization pipeline\n",
        "- Add the model and the evaluation function\n",
        "- Quantize the model\n",
        "- Output in in 'onnx/model_quantized.onnx'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72ef25eb",
      "metadata": {
        "id": "72ef25eb"
      },
      "outputs": [],
      "source": [
        "# Prepare the quantization pipeline\n",
        "build_dynamic_quant_yaml()\n",
        "# TODO: build the quantizer using the configuration defined before QUANT_CONF with the method Quantization\n",
        "quantizer = ...\n",
        "\n",
        "# Prepare dataset for quantization\n",
        "test_dataset = student_trainer.eval_dataset\n",
        "if student_trainer.column_mapping is not None:\n",
        "    test_dataset = trainer._apply_column_mapping(student_trainer.eval_dataset, student_trainer.column_mapping)\n",
        "    \n",
        "# Add model and evaluation function\n",
        "# TODO: Get the model stored in onnx/model.onnx \n",
        "quantizer.model = common.Model(...)\n",
        "\n",
        "# TODO: Setup the eval_func of the quantizer using the eval_func defined before\n",
        "quantizer.eval_func = functools.partial(...)\n",
        "\n",
        "# TODO: Launch quantization using quantizer()\n",
        "quantized_model = ...\n",
        "\n",
        "# TODO Output the model quantized to \"onnx/model_quantized.onnx\"\n",
        "quantized_model.save(...)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94938a76",
      "metadata": {
        "id": "94938a76"
      },
      "source": [
        "The final step consists in:\n",
        "- Loading the model quantized\n",
        "- Launching the benchmark\n",
        "- Adding the benchmark to all the benchmark\n",
        "- Printing the whole benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "695d16e2",
      "metadata": {
        "id": "695d16e2"
      },
      "outputs": [],
      "source": [
        "# TODO Load model quantized wusing onnx_path and mode_quantized.onnx as file_name\n",
        "ort_model = ORTModelForFeatureExtraction.from_pretrained(\n",
        "    ..., file_name= ...\n",
        ")\n",
        "# TODO: load onnx setfit model using the ort_model, the tokenizer and the student_model.model_head\n",
        "onnx_setfit_model = OnnxSetFitModel(..., ..., ...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8cdc6e3",
      "metadata": {
        "id": "d8cdc6e3"
      },
      "outputs": [],
      "source": [
        "# Launch the performance benchmark on the quantized model\n",
        "# TODO Launch benchmark for this distilled model onnx_setfit_model on the test_dataset\n",
        "pb = OnnxPerformanceBenchmark(\n",
        "    ...,\n",
        "    ...,\n",
        "    \"MiniLM-L3 (distilled + quantized)\",\n",
        "    model_path=\"onnx/model_quantized.onnx\",\n",
        "    metric_monitor = METRIC_MONITOR\n",
        ")\n",
        "# TODO: Launch the run_benchmark of the pb object\n",
        "perf_metrics.update(pb.run_benchmark())\n",
        "\n",
        "plot_metrics(perf_metrics, \"MiniLM-L3 (distilled + quantized)\", metric_monitor=METRIC_MONITOR)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d44a2f67",
      "metadata": {
        "id": "d44a2f67"
      },
      "source": [
        "# Zero Shot Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8WJaZ2Fze5gg",
      "metadata": {
        "id": "8WJaZ2Fze5gg"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "Zero-shot classification is a machine learning task where a model is expected to classify input data into classes that it has not seen or been trained on. This is in contrast to traditional supervised learning, where a model learns to classify input data based on examples of each class provided during the training process.\n",
        "\n",
        "The key idea behind zero-shot classification is that the model can generalize its knowledge to unseen classes by leveraging its understanding of the relationships between various concepts and the context of the input data. This makes zero-shot classification particularly useful in situations where it is difficult or impractical to obtain labeled training data for every possible class.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4960b70",
      "metadata": {
        "id": "a4960b70"
      },
      "outputs": [],
      "source": [
        "SEED = 412\n",
        "dataset_id = \"emotion\"\n",
        "TEST_DATA_SIZE = 100\n",
        "# TODO Load the dataset using load_dataset with the dataset_id as parameter\n",
        "reference_dataset = ...\n",
        "\n",
        "# TODO For time issue we want to sample the dataset using TEST_DATA_SIZE examples\n",
        "reference_dataset[\"test\"] = reference_dataset[\"test\"].shuffle(seed=SEED).select(range(...))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "367b8be3",
      "metadata": {
        "id": "367b8be3"
      },
      "outputs": [],
      "source": [
        "# Extract ClassLabel feature from \"label\" column\n",
        "# TODO: get the labels using the feature[\"label\"]  from the reference_dataset[\"train\"]\n",
        "label_features = ...\n",
        "# Label names to classify with\n",
        "candidate_labels = label_features.names"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7Stxnj6vhh53",
      "metadata": {
        "id": "7Stxnj6vhh53"
      },
      "source": [
        "\n",
        "\n",
        "The first thing we need to do is create a dataset of synthetic examples. In setfit, we can do this by applying the get_templated_dataset() function to a dummy dataset. This function expects a few main things:\n",
        "\n",
        "    A list of candidate labels to classify with. We'll use the labels from the reference dataset here, but this could be anything that's relevant to the task and dataset at hand.\n",
        "    A template to generate examples with. By default, it is \"This sentence is {}\", where the {} will be filled by one of the candidate labels\n",
        "    A sample size \n",
        "\n",
        ", which will create synthetic examples per class. We find\n",
        "\n",
        "    usually works best.\n",
        "\n",
        "Armed with this information, let's first extract some candidate labels from the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95356ca7",
      "metadata": {
        "id": "95356ca7"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "from setfit import get_templated_dataset\n",
        "\n",
        "# A dummy dataset to fill with synthetic examples\n",
        "dummy_dataset = Dataset.from_dict({})\n",
        "train_dataset = get_templated_dataset(dummy_dataset, candidate_labels=candidate_labels, sample_size=8)\n",
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5db17ea",
      "metadata": {
        "id": "f5db17ea"
      },
      "outputs": [],
      "source": [
        "from setfit import SetFitTrainer\n",
        "from setfit import SetFitModel\n",
        "# We will use the mpnet model here\n",
        "MODEL_ID = \"sentence-transformers/paraphrase-mpnet-base-v2\"\n",
        "# TODO Launch the setfitmodel using MODEL_ID with the SetFitModel.from_pretrained method\n",
        "model = ...\n",
        "\n",
        "# TODO Prepare the SetFitTrainer with the model on the train_dataset using reference_dataset[\"test\"]\n",
        "# as eval_dataset\n",
        "trainer = SetFitTrainer(\n",
        "    model=...,\n",
        "    train_dataset=...,\n",
        "    eval_dataset=...,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcddcf39",
      "metadata": {
        "id": "bcddcf39"
      },
      "outputs": [],
      "source": [
        "# TODO launch the training using the train method from the trainer\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pGYapXcaM7ib",
      "metadata": {
        "id": "pGYapXcaM7ib"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# TODO Evalute the trainer using the evaluate method from the trainer object\n",
        "zeroshot_metrics = ...\n",
        "zeroshot_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "feaa9877",
      "metadata": {
        "id": "feaa9877"
      },
      "outputs": [],
      "source": [
        "# Evaluate a bit more metrics from the trainer\n",
        "y_true, y_pred = evaluate_more_metrics(trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MnnnPY5vGhbD",
      "metadata": {
        "id": "MnnnPY5vGhbD"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import evaluate\n",
        "\n",
        "# TODO Load the \"zero-shot-classification\" pipeline\n",
        "pipe = pipeline(...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0gpyuQAaJBB4",
      "metadata": {
        "id": "0gpyuQAaJBB4"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Launch the predictions using the loaded pipeline\n",
        "zeroshot_preds = pipe(reference_dataset[\"test\"][\"text\"], batch_size=16, candidate_labels=candidate_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "536ce9f1",
      "metadata": {
        "id": "536ce9f1"
      },
      "outputs": [],
      "source": [
        "# Post process the predictions\n",
        "preds = [label_features.str2int(pred[\"labels\"][0]) for pred in zeroshot_preds]\n",
        "\n",
        "# TODO evaluate the \"accuracy\" metric\n",
        "metric = evaluate.load(...)\n",
        "\n",
        "# Compute the metrics and display those\n",
        "transformers_metrics = metric.compute(predictions=preds, references=reference_dataset[\"test\"][\"label\"])\n",
        "transformers_metrics"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "c5604f73-f395-42cb-8082-9974a87ef9e9"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "1a53731e204626af339a5238c341a3f8c4bfd7cb5ccdda48ca3fe8366eef4175"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
